[exp]
data_dir = /home/congtj/Unicorn/data/em-wa
output_dir = /nfs/turbo/coe-jag/congtj/openforge/finetuned_models/gemma-1.1-7b-it

[llm]
model_id = google/gemma-1.1-7b-it
num_train_epochs = 2
learning_rate = 5e-5
weight_decay = 0.01
train_batch_size = 8
eval_batch_size = 16
