[io]
input_dir = /nfs/turbo/coe-jag/congtj/openforge/deep_matcher_datasets/Structured/Walmart-Amazon/artifact
output_dir = /nfs/turbo/coe-jag/congtj/openforge/finetuned_models/qwen2.5-7b-instruct_lora/em_walmart-amazon

[llm]
model_id = Qwen/Qwen2.5-7B-Instruct
r = 8
lora_alpha = 16
lora_dropout = 0.1
num_train_epochs = 10
learning_rate = 2e-4
weight_decay = 0.01
train_batch_size = 16
eval_batch_size = 16
label_smoothing = 0.01
