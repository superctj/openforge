"""
Populating predictions and confidence scores for MRFs created in
'em-wa_create-mrfs_nv-embed-v2.py'.

Input directory and output directory are the same as we want to overwrite the 
placeholder predictions and confidence scores in the input files.
"""

import argparse
import os

import pandas as pd
import torch

from transformers import AutoModelForCausalLM, AutoTokenizer

from openforge.utils.custom_logging import create_custom_logger
from openforge.utils.llm_common import parse_llm_response
from openforge.utils.util import parse_config


PRIOR_MODEL_INSTRUCTION = """Entity matching is the task of determining whether two instances refer to the same real-world entity. For the following pair of entities, please determine if they refer to the same real-world entity. Return your prediction and confidence score in the following JSON format: '{"match": <true or false>, "confidence score": <Confidence score needs to be greater than 0.5 and smaller than 1.>}'. Give only the prediction and the confidence score. No explanation is needed.\n
"""  # noqa: E501


def get_llm_prediction(
    model,
    tokenizer,
    user_prompt: str,
    max_new_tokens: int = 20,
    device: str = "cuda",
    logger=None,
) -> int:
    messages = [
        {
            "role": "user",
            "content": user_prompt,
        },
    ]

    formatted_messages = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer(formatted_messages, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
        )

    # Only decode the part of the output that is generated by the model
    response = tokenizer.batch_decode(
        outputs[:, inputs["input_ids"].shape[1] :], skip_special_tokens=True
    )[0]

    if logger:
        logger.info(f"Prompt: {user_prompt}")
        logger.info(f"Response: {response}")

    pred, confdc_score = parse_llm_response(response, keyword="match")

    return pred, confdc_score


def run_prior_inference(
    model,
    tokenizer,
    max_new_tokens: int,
    num_retries: int,
    input_dir: str,
    device,
):
    for f in os.listdir(input_dir):
        if f.endswith(".json"):
            input_df = pd.read_json(os.path.join(input_dir, f))

            # This row is in the original dataset and has prior prediction and
            # confidence score
            first_row = input_df.iloc[0]
            all_predictions = [first_row["prediction"]]
            all_confdc_scores = [first_row["confidence_score"]]

            # Drop placeholder columns
            input_df = input_df.drop(columns=["prediction", "confidence_score"])

            for i, row in input_df.iterrows():
                # Skip the first row as it is in the original dataset
                if i == 0:
                    continue

                prompt = (
                    PRIOR_MODEL_INSTRUCTION
                    + f"Input:\nInstance 1: {row['l_entity']}\nInstance 2: {row['r_entity']}\n\nOutput:"  # noqa: E501
                )

                confdc_score = -1
                num_attempts = 0

                while confdc_score < 0.5 or confdc_score >= 1:
                    pred, confdc_score = get_llm_prediction(
                        model, tokenizer, prompt, max_new_tokens, device, logger
                    )

                    num_attempts += 1
                    if num_attempts >= num_retries:
                        confdc_score = 0.6
                        break

                all_predictions.append(pred)
                all_confdc_scores.append(confdc_score)

                logger.info(f"prediction: {pred}")
                logger.info(f"confidence score: {confdc_score}")
                logger.info("-" * 80)
                # if i >= 2:  # for testing
                #     exit(0)

            input_df["prediction"] = all_predictions
            input_df["confidence_score"] = all_confdc_scores

            # Overwrite the original file with populated predictions and
            # confidence scores
            output_filepath = os.path.join(input_dir, f)
            input_df.to_json(output_filepath, orient="records", indent=4)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--config_path",
        type=str,
        help="Path to the experiment configuration file",
    )

    args = parser.parse_args()

    config = parse_config(args.config_path)

    # Create logger
    output_dir = config.get("io", "output_dir")  # Output_dir should exist
    valid_output_dir = os.path.join(output_dir, "validation")
    test_output_dir = os.path.join(output_dir, "test")

    logger = create_custom_logger(output_dir)
    logger.info(f"Running program: {__file__}\n")
    logger.info(f"{args}\n")
    printable_config = {section: dict(config[section]) for section in config}
    logger.info(f"Experiment configuration:\n{printable_config}\n")

    # Populate prior predictions and confidence scores for MRFs
    prior_model_id = config.get("prior", "model_id")
    max_new_tokens = config.getint("prior", "max_new_tokens")
    num_retries = config.getint("prior", "num_retries")

    prior_model = AutoModelForCausalLM.from_pretrained(
        prior_model_id, torch_dtype="auto", trust_remote_code=True
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    prior_model = prior_model.to(device)
    tokenizer = AutoTokenizer.from_pretrained(
        prior_model_id, trust_remote_code=True
    )

    run_prior_inference(
        prior_model,
        tokenizer,
        max_new_tokens,
        num_retries,
        valid_output_dir,
        device,
    )

    run_prior_inference(
        prior_model,
        tokenizer,
        max_new_tokens,
        num_retries,
        test_output_dir,
        device,
    )
