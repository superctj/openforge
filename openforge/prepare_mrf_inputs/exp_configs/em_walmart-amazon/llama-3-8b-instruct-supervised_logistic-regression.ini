[io]
prior_dir = /nfs/turbo/coe-jag/congtj/openforge/deep_matcher_datasets/Structured/Walmart-Amazon/artifact/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised/logistic_regression/hp_tuning
prior_model_filepath = /nfs/turbo/coe-jag/congtj/openforge/deep_matcher_datasets/Structured/Walmart-Amazon/artifact/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised/logistic_regression/hp_tuning/logistic_regression.pkl
output_dir = /nfs/turbo/coe-jag/congtj/openforge/deep_matcher_datasets/Structured/Walmart-Amazon/artifact/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised/logistic_regression/hp_tuning/mrf_inputs

[encoding]
base_model_id = McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp
lora_model_id = McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised
pooling_mode = mean
max_length = 1024
batch_size = 32

[knn]
n_neighbors = 5